Lab 6 - Bardi - Tara and Hanieh

Translation
  We started with our original test case of sentence #2 "Dogs chase cars" that
  produced 19419 results. We updated semi.vpm for tense (all we had to do was
  remove the E from the feature path since Bardi only has past, present, and
  future tenses):
    E.TENSE : TENSE
      pst <> past
      prs <> present
      fut <> future
      * <> *
  This yielded 3072 results. We then updated semi.vpm for aspect. In addition
  to removing the E from the feature path, we also added a no-aspect aspect to
  the grammar. We did this because aspect is marked for some sentences but can
  be left undrespecified. We also included the appropriate lines semi.vpm (as
  described in the lab 6 instructions):
    E.ASPECT : ASPECT
      cont <> cont
      rem <> rem
      rec <> rec
      pfv <> pfv
      sim <> sim
      * >> no-aspect
      no-aspect << [e]
  This yielded 768 results. Next, we updated semi.vpm for pernum, making sure
  to have the appropriate values on each side (this took some trial and error
  until we got the syntax right):
    PNG.PERNUM : PER NUM
      3aug <> third pl
      2aug <> second pl
      1aug <> first pl
      3min <> third sg
      2min <> second sg
      1min_excl <> first sg
      1min_incl <> first pl
      1min <> first *
      3rd <> third *
      2nd <> second *
      1st <> first *
      aug <> pl *
      min <> sg *
  This yielded 192 results. From here we moved on to editing the lexicon and
  tdl files to reduce over-generation. As suggested, we removed iil (dog_n_rel)
  and just used iila for dog_n_rel. This yielded 96 results. It was also
  suggested that we combine the different prefixes for single inflectional
  rules. For example, we combined
    aug-prefix1 :=
    %prefix (* arr-)
    aug-lex-rule.

    aug-prefix2 :=
    %prefix (* rr-)
    aug-lex-rule.
  into one rule:
    aug-prefix1 :=
    %prefix (* arr-) (* rr-)
    aug-lex-rule.
  We did this with all of the inflectional rules and produced 12 results. At
  this point we tested sentence #4 (Dogs eat) which resulted in 52 results. We
  removed all variations of arli (eat_v_rel) from the lexicon and yielded 10
  results. Now, the only factors producing this number of results was the fact
  that Bardi has free word-order (6 different possibilities) and that the
  transitive marker was optional (2 possibilities). Based off of what was shown
  during demo day and the files sent as a result (thank you!), we were able to
  constrain the transitivity marker using the customization system. We did this
  using require and forbid constraints. Transitive verbs required the tr-min or
  tr-aug position classes and intransitive verbs forbade them. For example,
    verb1_name=it-abs
      verb1_feat1_name=case
      verb1_feat1_value=abs
      verb1_feat1_head=subj
    verb1_valence=intrans
      verb1_stem1_orth=jarrmi
      verb1_stem1_pred=_sleep_v_rel
      verb1_stem2_orth=banya
      verb1_stem2_pred=_finish_v_rel
      verb1_stem3_orth=lirrmi
      verb1_stem3_pred=_call.out_v_rel
      verb1_stem4_orth=ondi
      verb1_stem4_pred=_cover_v_rel
      verb1_stem5_orth=a
      verb1_stem5_pred=_take_v_rel
      verb1_stem6_orth=al
      verb1_stem6_pred=_be_v_rel
      verb1_stem7_orth=jala
      verb1_stem7_pred=_see_v_rel
      verb1_stem8_orth=jargi
      verb1_stem8_pred=_fear_v_rel
      verb1_stem9_orth=arli
      verb1_stem9_pred=_eat_v_rel
      verb1_forbid1_others=verb-pc5
      verb1_forbid2_others=verb-pc6
    verb2_name=tr-erg-abs
      verb2_feat1_name=case
      verb2_feat1_value=erg
      verb2_feat1_head=subj
      verb2_feat2_name=case
      verb2_feat2_value=abs
      verb2_feat2_head=obj
    verb2_valence=trans
      verb2_stem1_orth=nya
      verb2_stem1_pred=_catch_v_rel
      verb2_stem2_orth=ny
      verb2_stem2_pred=_catch_v_rel
      verb2_stem3_orth=inya
      verb2_stem3_pred=_catch_v_rel
      verb2_stem4_orth=jala
      verb2_stem4_pred=_see_v_rel
      verb2_stem5_orth=loonga
      verb2_stem5_pred=_collect_v_rel
      verb2_stem6_orth=arli
      verb2_stem6_pred=_eat_v_rel
      verb2_stem11_orth=alboo
      verb2_stem11_pred=_dig_v_rel
      verb2_stem12_orth=boo
      verb2_stem12_pred=_spear_v_rel
      verb2_stem13_orth=jal
      verb2_stem13_pred=_look.at_v_rel
      verb2_stem14_orth=b
      verb2_stem14_pred=_poke_v_rel
      verb2_stem15_orth=joogool
      verb2_stem15_pred=_break_v_rel
      verb2_stem16_orth=joogooloo
      verb2_stem16_pred=_break_v_rel
      verb2_stem17_orth=jooloong
      verb2_stem17_pred=_pick.up_v_rel
      verb2_stem18_orth=boo
      verb2_stem18_pred=_hit_v_rel
      verb2_stem19_orth=ngoorribi
      verb2_stem19_pred=_chase_v_rel
      verb2_stem20_orth=jangarrga
      verb2_stem20_pred=_ask_v_rel
      verb2_stem21_orth=yarral
      verb2_stem21_pred=_run_v_rel
      verb2_stem22_orth=ngajim
      verb2_stem22_pred=_hit_v_rel
      verb2_require1_others=verb-pc5, verb-pc6
  This brought us down to 6 results for sentence #2 and #4! We also tested
  sentence #19 (My dogs sleep) and got 4 results.

Phenomena
  Possession
    We worked on getting both possessive sentences in eng.txt to translate. We
    started with #19 (My dogs sleep) as we already had the sentence parsing in the
    lkb:
      Ngajana iila irrjarrmi.
      ngajana iila i-rr-jarrmi
      1MIN.POSS dog 3-AUG-get.up
      `My dogs sleep.'
    Bardi has free possessor pronouns that are marked for
    both person and number. We implemented this during week 4 using a class of
    possessor pronouns:
      poss-pron1_type=non-affix
      poss-pron1_agr=non-agree
      poss-pron1_mod-spec=spec
      poss-pron1_order=head-final
        poss-pron1_instance1_name=jana
        poss-pron1_instance1_orth=jana
          poss-pron1_instance1_feat1_name=pernum
          poss-pron1_instance1_feat1_value=1min_excl
        poss-pron1_instance2_name=jiya
        poss-pron1_instance2_orth=jiya
          poss-pron1_instance2_feat1_name=pernum
          poss-pron1_instance2_feat1_value=2min
        poss-pron1_instance3_name=jina
        poss-pron1_instance3_orth=jina
          poss-pron1_instance3_feat1_name=pernum
          poss-pron1_instance3_feat1_value=3min
        poss-pron1_instance5_name=jowa
        poss-pron1_instance5_orth=jowa
          poss-pron1_instance5_feat1_name=pernum
          poss-pron1_instance5_feat1_value=1min_incl
        poss-pron1_instance6_name=jarda
        poss-pron1_instance6_orth=jarda
          poss-pron1_instance6_feat1_name=pernum
          poss-pron1_instance6_feat1_value=1aug
        poss-pron1_instance7_name=joogarra
        poss-pron1_instance7_orth=joogarra
          poss-pron1_instance7_feat1_name=pernum
          poss-pron1_instance7_feat1_value=2aug
        poss-pron1_instance8_name=jirra
        poss-pron1_instance8_orth=jirra
          poss-pron1_instance8_feat1_name=pernum
          poss-pron1_instance8_feat1_value=3aug
      poss-pron1_possessum-mark=no
    We were able to get 2 results when doing eng to bcj translation on #19 (since
    Bardi has free word-order). Sentence #18 (The dog's car sleeps) was more
    difficult to get parsing, let alone translating. We first tried to get the
    following sentence to parse:
      Iila jina yandilybar irrjarrmi.
      iila jina yandilybar i-rr-jarrmi
      dog 3MIN.POSS car 3-AUG-sleep
      `The dog's car sleeps. (lit. dog his car sleeps)'
    We were able to get it to parse by adding a two new possessive strategies
    (one for min and one for aug):
      poss-strat1_order=head-final
      poss-strat1_mod-spec=spec
      poss-strat1_mark-loc=possessor
      poss-strat1_pronoun-allow=no
      poss-strat1_possessor-type=non-affix
      poss-strat1_possessor-mark-order=head-final
        poss-strat1_dep-comp-feat1_name=pernum
        poss-strat1_dep-comp-feat1_value=min
      poss-strat1_possessor-agr=non-agree
      poss-strat1_possessor-orth=jina
      poss-strat2_order=head-final
      poss-strat2_mod-spec=spec
      poss-strat2_mark-loc=possessor
      poss-strat2_pronoun-allow=no
      poss-strat2_possessor-type=non-affix
      poss-strat2_possessor-mark-order=head-final
        poss-strat2_dep-comp-feat1_name=pernum
        poss-strat2_dep-comp-feat1_value=aug
      poss-strat2_possessor-agr=non-agree
      poss-strat2_possessor-orth=jirra
    This allowed #18 to parse however, only 1 of the 3 parses was correct. 2 of
    the parses built the sentence using the wh-ques grammar rule. To fix this,
    we constrained the possessive adpositions to have empty NON-LOCAL values
    (as advised on Canvas):
      two-rel-adposition-lex := basic-icons-lex-item &
        [ SYNSEM [ NON-LOCAL non-local-none,
                  LOCAL [ CAT [ HEAD adp,
                                VAL.COMPS < [ LOCAL [ CAT cat-sat,
                                                      CONT.HOOK #hook &
                                                                [ INDEX #ind,
                                                                  ICONS-KEY.IARG1 #clause ] ] ] > ],
                          CONT.HOOK #hook &
                                    [ CLAUSE-KEY #clause ] ],
                  LKEYS.KEYREL arg12-ev-relation &
                                [ ARG2 #ind ] ] ].
    After fixing this, #18 resulted in one parse.

  Object Dropping
    In Bardi, personal pronoun objects are marked on the verb. Bowern refers to
    these as direct object clitics (pp 402) however we have implemented them as
    affixes. They always take the last position in the verb. For example,
      Iilanim irroongoorribiirr.
      iila-nim i-rr-oo-ngoorribi-irr
      dog-ERG 3-AUG-TR-chase-3AUG.DO
      `Dogs chase them.'
    We added a verb position class for all of the different pernum combination
    lexical rules:
      verb-pc7_name=dir-obj
      verb-pc7_order=suffix
      verb-pc7_inputs=verb-pc1, verb-pc3
        verb-pc7_require1_others=verb2, verb6, verb8
        verb-pc7_lrt1_name=1min-dir-obj
          verb-pc7_lrt1_feat1_name=pernum
          verb-pc7_lrt1_feat1_value=1min_excl
          verb-pc7_lrt1_feat1_head=obj
          verb-pc7_lrt1_lri1_inflecting=yes
          verb-pc7_lrt1_lri1_orth=-ngay
        verb-pc7_lrt2_name=2min-dir-obj
          verb-pc7_lrt2_feat1_name=pernum
          verb-pc7_lrt2_feat1_value=2min
          verb-pc7_lrt2_feat1_head=obj
          verb-pc7_lrt2_lri1_inflecting=yes
          verb-pc7_lrt2_lri1_orth=-rri
        verb-pc7_lrt3_name=3min-dir-obj
          verb-pc7_lrt3_feat1_name=pernum
          verb-pc7_lrt3_feat1_value=3min
          verb-pc7_lrt3_feat1_head=obj
          verb-pc7_lrt3_lri1_inflecting=no
        verb-pc7_lrt4_name=1plus2min-dir-obj
          verb-pc7_lrt4_feat1_name=pernum
          verb-pc7_lrt4_feat1_value=1min_incl
          verb-pc7_lrt4_feat1_head=obj
          verb-pc7_lrt4_lri1_inflecting=yes
          verb-pc7_lrt4_lri1_orth=-way
        verb-pc7_lrt5_name=1aug-dir-obj
          verb-pc7_lrt5_feat1_name=pernum
          verb-pc7_lrt5_feat1_value=1aug
          verb-pc7_lrt5_feat1_head=obj
          verb-pc7_lrt5_lri1_inflecting=yes
          verb-pc7_lrt5_lri1_orth=-moord
        verb-pc7_lrt6_name=2aug-dir-obj
          verb-pc7_lrt6_feat1_name=pernum
          verb-pc7_lrt6_feat1_value=2aug
          verb-pc7_lrt6_feat1_head=obj
          verb-pc7_lrt6_lri1_inflecting=yes
          verb-pc7_lrt6_lri1_orth=-goorr
        verb-pc7_lrt7_name=3aug-dir-obj
          verb-pc7_lrt7_feat1_name=pernum
          verb-pc7_lrt7_feat1_value=3aug
          verb-pc7_lrt7_feat1_head=obj
          verb-pc7_lrt7_lri1_inflecting=yes
          verb-pc7_lrt7_lri1_orth=-irr
    We also changed the arg-opt options to:
      obj-drop=obj-drop-all
      obj-mark-drop=obj-mark-drop-opt
      obj-mark-no-drop=obj-mark-no-drop-opt
    We attempted to parse the sentence above but were unable to get it to work.
    This was because we were initially missing the dashes in some of the
    suffixes. After fixing this, we were able to get both pronoun objects that
    are marked on the verb as well as object dropping sentences like:
      Iil oorrarli.
      iila oo-rr-arli
      dog.ABS 3-AUG-eat
      `Dogs eat.'
    to parse.

  Negation
    In Bardi, there is a negation marker "arra" that precedes the verb. In
    addition to the marker, the verb is marked for irrealis, which takes the
    same position as tense does. Free word-order still applies so elements can
    appear between the negation marker and the verb however, it is more common
    to find the negation marker at the beginning of the sentence, followed by
    the verb (either a simple verb or a preverb + verb complex predicate),
    followed by the rest of the sentence. The example given by Bowern on pp 509,
    with all being acceptable but the first being preferred:
      1. Arra joodarrarr o-l-ala-na.
         NEG with.the.tide 3-IRR-visit-REM.PST
         `He didnâ€™t go with the tide.'
      2. Joodarrarr arra o-l-ala-na.
         with.the.tide NEG 3-IRR-visit-REM.PST
      3. Arra o-l-ala-na joodarrarr
         NEG 3-IRR-visit-REM.PST with.the.tide
    We used sentence #5 to test our implementation:
      Arra iilanim irroongoorribi yandilybar.
      arra iila-nim i-la-rr-oo-ngoorribi yandilybar
      NEG dog-ERG 3-IRR-AUG-TR-chase boat
      `The dogs don't chase boats.'
    We implemented the negation marker using the customization system. We added
    it as an adverb like particle because that matched the closest to how
    Bowern described its use:
      section=sentential-negation
      neg-exp=1
      comp-neg=on
      comp-neg-head=v
      comp-neg-order=before
      comp-neg-orth=arra
    We also added the irrealis mood to the tense position class, since it takes
    the same spot in the verb marking. We were able to get #5 to parse however,
    were not able to figure out how to constrict negation to require irrealis.
    After getting some feedback on Canvas, we switched to bipartite negation,
    with the first part being a free, syntactic complement (arra) and the second
    part being a bound morpheme:
      section=sentential-negation
      neg-exp=2
      infl-neg=on
      adv-neg=on
      comp-neg=on
      comp-neg-head=v
      comp-neg-order=before
      comp-neg-orth=arra
      bineg-type=infl-comp
      neg1-type=fc
      neg2-type=b
      comp-neg-head-infl-comp-neg=v
      comp-neg-order-infl-head=before
    For the irrealis restriction, we added an irrealis marker to the tense
    position class and gave it the negation plus feature:
      verb-pc2_lrt5_name=irr
        verb-pc2_lrt5_feat1_name=mood
        verb-pc2_lrt5_feat1_value=irr
        verb-pc2_lrt5_feat1_head=verb
        verb-pc2_lrt5_feat2_name=negation
        verb-pc2_lrt5_feat2_value=plus
        verb-pc2_lrt5_feat2_head=verb
        verb-pc2_lrt5_lri1_inflecting=yes
        verb-pc2_lrt5_lri1_orth=la-
    This allowed for sentence #5 to parse and sentences like:
      Arra iilanim irroongoorribi yandilybar.
      arra iila-nim i-rr-oo-ngoorribi yandilybar
      NEG dog-ERG 3-AUG-TR-chase boat
      `*The dogs don't chase boats.
    which does not have the irrealis marker, to not parse.
    (Note from Tara: I know we weren't supposed to work on negation this week
    but I accidentally fell down a bit of a rabbit hole and ended up working on
    it.)

  More Translation
    After further implementing some of the phenomena above, we went through and
    translated any sentences that we could. Below is a summary:
      #1: 2 results
      #2: 12 results (possibly the result of implementation of object dropping
        this is explained more as a new ambiguity source below)
      #3: 0 results (expected at this point)
      #4: 2 results
      #5: 24 results (first test after adding negation)
      #18: 0 results (it parses in lkb but won't generate most likely because
        Bardi doesn't have determiners for nouns like "the" or "a"; it goes from
        "the dog" in English to "dog" in Bardi)
      #19: 2 results

TSDB
  Initial Run
    corpus (tsdb/home/bardi/lab6/lab5grammar/corpus)
      1. 0 items parsed
      2. We averaged 0 parses per parsed item.
      3. Our most ambiguous item got 0 readings.

    testsuite (tsdb/home/bardi/lab6/lab5grammar/lab6)
      1. 13 items parsed
      2. We averaged ~2 parses per parsed item.
      3. Our most ambiguous item got 6 parses.
      4. Our most ambiguous item has 6 parses due to different combinations of
        the EX-ADJ and EX-SUBJ rules applying to the VP formed by the COMP-HEAD
        rule in this sentence:
          Ngamarlang nganangajimgal.
          nga-marla-ng nga-na-ngajim-gal
          1MIN.POSS-hand-INS 1-TR.PST-hit-REC.PST
          `I hit him with my hand.'
        This is an example of the possessive marker on an inalienable noun. We
        are not entirely sure why this is happening or really what those 2 rules
        are doing.

  Final Run
    corpus (tsdb/home/bardi/lab6/lab6grammar/corpus)
      1. 0 items parsed
      2. We averaged 0 parses per parsed item.
      3. Our most ambiguous item got 0 readings.

    testsuite (tsdb/home/bardi/lab6/lab6grammar/lab6)
      1. 12 items parsed
      2. We averaged ~2 parses per parsed item.
      3. Our most ambiguous item(s) got 4 parses.
      4. A new source of ambiguity is with the new direct object lexical rules.
        Since the 3min direct object marker is null, it is getting applied to
        a lot of sentences that shouldn't have it. In many cases, when there is
        an overt direct object, no marker is needed. However, there are also
        cases where a marker is allowed. I'm not entirely sure how to fix this.

  Coverage
    - Initial Run
      - corpus: 0% coverage
      - testsuite: 16.2% coverage
    - Final Run
      - corpus: 0% coverage
      - testsuite: 14.9% coverage
    - Comparison
      - corpus: We did not put any focus into updating our lexicon to include words
        from the corpus. Additionally, in an attempt to improve ambiguity when
        translating, we merged most morphological affixes that had the same
        purpose. We have been trying to update our testsuite to reflect these
        changes but have not updated the corpus. Are we expected to?
      - testsuite: Sadly, our coverage went down. In an attempt to get sentences
        to translate with less results, we combined some morphology and lexical
        entries. We tried our best to update the testsuite along with this
        changes but it appeared we missed a couple of entries which is why a
        few of our sentences are no longer parsing.